{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring `fastcore.script`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##| export\n",
    "from fastcore.all import *\n",
    "@call_parse\n",
    "def main(msg:str,     # The message\n",
    "         upper:bool): # Convert to uppercase?\n",
    "    'Print `msg`, optionally converting to uppercase'\n",
    "    print(msg.upper() if upper else msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: core.py [-h] [--upper] msg\n",
      "\n",
      "Print `msg`, optionally converting to uppercase\n",
      "\n",
      "positional arguments:\n",
      "  msg         The message\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n",
      "  --upper     Convert to uppercase? (default: False)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "! python ../fsdp_qdora_the_explorer/core.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "! python ../fsdp_qdora_the_explorer/core.py hello --upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcall_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Decorator to create a simple CLI from `func` using `anno_parser`\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/default/lib/python3.11/site-packages/fastcore/script.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall signature:\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           partial\n",
      "\u001b[0;31mString form:\u001b[0m    functools.partial(<function xt>, 'param')\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniforge3/envs/default/lib/python3.11/functools.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "partial(func, *args, **keywords) - new function with partial application\n",
      "of the given arguments and keywords."
     ]
    }
   ],
   "source": [
    "?Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##| export\n",
    "@call_parse\n",
    "def main(msg:Param('The message', str),\n",
    "         upper:Param('Convert to uppercase?', store_true)):\n",
    "    'Print `msg`, optionally converting to uppercase'\n",
    "    print(msg.upper() if upper else msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: core.py [-h] [--upper] msg\n",
      "\n",
      "Print `msg`, optionally converting to uppercase\n",
      "\n",
      "positional arguments:\n",
      "  msg         The message\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n",
      "  --upper     Convert to uppercase? (default: False)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "! python ../fsdp_qdora_the_explorer/core.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function main in module __main__:\n",
      "\n",
      "main(msg: ['param', ('The message', <class 'str'>), {}], upper: ['param', ('Convert to uppercase?', <function store_true>), {}])\n",
      "    Print `msg`, optionally converting to uppercase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(required:Param('Required param', int),\n",
    "      a:Param('param 1', bool_arg),\n",
    "      b:Param('param 2', str)='test'):\n",
    "    'my_docs'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function f in module __main__:\n",
      "\n",
      "f(required: ['param', ('Required param', <class 'int'>), {}], a: ['param', ('param 1', <function bool_arg>), {}], b: ['param', ('param 2', <class 'str'>), {}] = 'test')\n",
      "    my_docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(action:Param('Action', str_enum('settings', 'train'))):\n",
    "  match action:\n",
    "    case 'settings': print('Settings')\n",
    "    case 'train': print('Training')\n",
    "    case _: print('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'param'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0menum\u001b[0m \u001b[0;34m'settings'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_85048/3984288533.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function main in module __main__:\n",
      "\n",
      "main(action: ['param', ('Action', <enum 'settings'>), {}])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings\n"
     ]
    }
   ],
   "source": [
    "main('settings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "main('lobby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def settings(\n",
    "    world_size: Param('Number of GPUs to use. -1 = all available GPUs.', int) = -1,\n",
    "    train_type: Param(\"Training type\", choices=[\"full\", \"lora\", \"qlora\", \"custom_qlora\", \"custom_lora\", \"hqq_lora\", \"hqq_dora\", \"bnb_dora\", \"bnb_llama_pro\", \"hqq_llama_pro\"]) = \"qlora\",\n",
    "    llama_pro_path: Param('Path to the quantized llama pro model', str) = None,\n",
    "    batch_size: Param('Batch size per GPU. Effective BS = batch_size * world_size * gradient_accumulation_steps', int) = 1,\n",
    "    context_length: Param('Max length of input sequence (in tokens)', int) = 512,\n",
    "    gradient_accumulation_steps: Param('How many steps to accumulate gradients over (increases effective batch size)', int) = 1,\n",
    "    num_epochs: Param('How many epochs of training to do', int) = 1,\n",
    "    dataset: Param(\"Dataset to use\", choices=[\"alpaca\", \"alpaca_sample\", \"dummy\", \"guanaco\", \"sql\", \"orca_math\"]) = \"alpaca_sample\",\n",
    "    dataset_samples: Param('Number of samples in an epoch if using \"alpaca_sample\" or \"dummy\" dataset', int) = 512,\n",
    "    sharding_strategy: Param(\"Sharding strategy for FSDP\", choices=[\"full_shard\", \"shard_grad_op\", \"ddp\", \"hybrid_full_shard\", \"hybrid_shard_grad_op\"]) = \"full_shard\",\n",
    "    use_gradient_checkpointing: Param('Use FSDP\\'s activation checkpointing', bool) = True,\n",
    "    reentrant_checkpointing: Param('Use re-entrant autograd activation checkpointing. Setting to True can use less GPU memory with BNB QLoRA', bool) = False,\n",
    "    use_cpu_offload: Param('Use FSDP\\'s CPU offloading', bool) = True,\n",
    "    use_activation_cpu_offload: Param('Use FSDP\\'s activation CPU offloading', bool) = False,\n",
    "    low_memory: Param('Load one copy of the model into CPU memory before sharding with FSDP. For QLoRA, quantizes each layer individually on GPU before placing on CPU.', bool) = True,\n",
    "    no_sync: Param('Prevent gradient sync until update step. Likely uses more memory. Required for `use_cpu_offload` and `gradient_accumulation_steps > 1`', bool) = False,\n",
    "    precision: Param(\"Training precision\", choices=[\"fp32\", \"bf16\", \"fp16_autocast\", \"bf16_autocast\", \"bf16_buffers_autocast\"]) = \"bf16\",\n",
    "    model_name: Param('Which model to train - e.g. \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"', str) = \"meta-llama/Llama-2-7b-hf\",\n",
    "    save_model: Param('Save the resulting model', bool) = False,\n",
    "    output_dir: Param('Output directory to save the final model to', str) = \"output\",\n",
    "    lora_rank: Param('LoRA rank for lora/qlora', int) = 64,\n",
    "    lora_alpha: Param('LoRA alpha for lora/qlora', int) = 16,\n",
    "    lora_dropout: Param('LoRA dropout for lora/qlora', float) = 0.1,\n",
    "    lora_target_modules: Param(\"LoRA target modules\", choices=[\"all\", \"default\"]) = \"all\",\n",
    "    verbose: Param('Whether to print extra info for debugging', bool) = False,\n",
    "    lr: Param('Learning rate', float) = 1e-5,\n",
    "    apply_gradient_clipping: Param('Apply gradient norm clipping', bool) = False,\n",
    "    grad_norm: Param('Gradient norm clipping', float) = 0.3,\n",
    "    wd: Param('Weight decay', float) = 0.1,\n",
    "    profile_memory: Param('Profile memory usage for the first few batches. Keep false for training. May increase memory usage.', bool) = False,\n",
    "    optimizer: Param(\"Optimizer\", choices=[\"adamw\", \"adam\", \"sgd\", \"adadelta\"]) = \"adamw\",\n",
    "    lr_scheduler: Param(\"Learning Rate Scheduler\", choices=[\"constant\", \"linear\", \"cosine\"]) = \"constant\",\n",
    "    loading_workers: Param('Number of layers to load and quantize in parallel per GPU. Default of -1 uses heuristics to set worker count.', int) = -1,\n",
    "    log_to: Param(\"Where to log output\", choices=[\"tqdm\", \"wandb\", \"stdout\"]) = \"tqdm\",\n",
    "    master_addr: Param('For distributed training', str) = \"localhost\",\n",
    "    master_port: Param('For distributed training, must be the same for all processes', str) = \"12355\",\n",
    "    seed: Param('Random seed', int) = 42,\n",
    "    project_name: Param('For wandb logging', str) = \"fsdp_qlora\",\n",
    "    name: Param('For wandb logging', str) = None,\n",
    "    group: Param('For wandb logging', str) = None,\n",
    "    entity: Param('For wandb logging', str) = None,\n",
    "    n_bits: Param('passed to hqq', int) = 4,\n",
    "    profile: Param('Whether to profile with torch.profiler', bool) = False,\n",
    "    profiling_output: Param('Output file prefix for profiling', str) = \"profiles\",\n",
    "    with_stack: Param('Output stacks for profiling. Note that setting export_memory_timeline will automatically export traces since `with_stack` must be true to profile memory.', bool) = False,\n",
    "    with_shapes: Param('Output shapes for profiling. Can impact performance. Note that setting export_memory_timeline will automatically export traces since `with_shapes` must be true to profile memory.', bool) = False,\n",
    "    export_trace: Param('Output trace for profiling', bool) = True,\n",
    "    export_memory_timeline: Param('Output memory timeline for profiling', bool) = False,\n",
    "    wait_steps: Param('Wait steps when running profiler. Only used if repeat != 0.', int) = 0,\n",
    "    warmup_steps: Param('Warmup steps when running profiler', int) = 1,\n",
    "    active_steps: Param('Active steps when running profiler', int) = 2,\n",
    "    repeat: Param('Number of profiler cycles (wait + warmup + active) if > 0, else repeats forever', int) = 0,\n",
    "    profiling_frequency: Param('Profiling frequency in steps. Only used if repeat == 0, in which case wait_steps will be set to profiling_frequency - (warmup_steps + active_steps) such that the effective cycle length == profiling_frequency', int) = 10,\n",
    "    max_steps: Param('Max number of training steps (in units of batches) per epoch. -1 means no max_steps, otherwise training loop breaks after `max_steps` each epoch', int) = -1,\n",
    "):\n",
    "  print(f'{locals()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'world_size': -1, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 512, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'dataset': 'alpaca_sample', 'dataset_samples': 512, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': True, 'reentrant_checkpointing': False, 'use_cpu_offload': True, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'meta-llama/Llama-2-7b-hf', 'save_model': False, 'output_dir': 'output', 'lora_rank': 64, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': False, 'lr': 1e-05, 'apply_gradient_clipping': False, 'grad_norm': 0.3, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'constant', 'loading_workers': -1, 'log_to': 'tqdm', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1}\n"
     ]
    }
   ],
   "source": [
    "settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
