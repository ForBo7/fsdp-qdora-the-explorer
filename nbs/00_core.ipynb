{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(): pass\n",
    "def train(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = 'fsdp_qdora.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, description: str, param_type: type, default: any, choices: List[str] = None):\n",
    "        self.description = description\n",
    "        self.type = param_type\n",
    "        self.value = default\n",
    "        self.choices = choices\n",
    "\n",
    "class DefaultParameters:\n",
    "    def __init__(self):\n",
    "        self.world_size = Parameter('Number of GPUs to use. -1 = all available GPUs.', int, -1)\n",
    "        self.train_type = Parameter(\"Training type\", str, \"qlora\", choices=[\"full\", \"lora\", \"qlora\", \"custom_qlora\", \"custom_lora\", \"hqq_lora\", \"hqq_dora\", \"bnb_dora\", \"bnb_llama_pro\", \"hqq_llama_pro\"])\n",
    "        self.llama_pro_path = Parameter('Path to the quantized llama pro model', str, None)\n",
    "        self.batch_size = Parameter('Batch size per GPU. Effective BS = batch_size * world_size * gradient_accumulation_steps', int, 1)\n",
    "        self.context_length = Parameter('Max length of input sequence (in tokens)', int, 512)\n",
    "        self.gradient_accumulation_steps = Parameter('How many steps to accumulate gradients over (increases effective batch size)', int, 1)\n",
    "        self.num_epochs = Parameter('How many epochs of training to do', int, 1)\n",
    "        self.dataset = Parameter(\"Dataset to use\", str, \"alpaca_sample\", choices=[\"alpaca\", \"alpaca_sample\", \"dummy\", \"guanaco\", \"sql\", \"orca_math\"])\n",
    "        self.dataset_samples = Parameter('Number of samples in an epoch if using \"alpaca_sample\" or \"dummy\" dataset', int, 512)\n",
    "        self.sharding_strategy = Parameter(\"Sharding strategy for FSDP\", str, \"full_shard\", choices=[\"full_shard\", \"shard_grad_op\", \"ddp\", \"hybrid_full_shard\", \"hybrid_shard_grad_op\"])\n",
    "        self.use_gradient_checkpointing = Parameter('Use FSDP\\'s activation checkpointing', bool, True)\n",
    "        self.reentrant_checkpointing = Parameter('Use re-entrant autograd activation checkpointing. Setting to True can use less GPU memory with BNB QLoRA', bool, False)\n",
    "        self.use_cpu_offload = Parameter('Use FSDP\\'s CPU offloading', bool, True)\n",
    "        self.use_activation_cpu_offload = Parameter('Use FSDP\\'s activation CPU offloading', bool, False)\n",
    "        self.low_memory = Parameter('Load one copy of the model into CPU memory before sharding with FSDP. For QLoRA, quantizes each layer individually on GPU before placing on CPU.', bool, True)\n",
    "        self.no_sync = Parameter('Prevent gradient sync until update step. Likely uses more memory. Required for `use_cpu_offload` and `gradient_accumulation_steps > 1`', bool, False)\n",
    "        self.precision = Parameter(\"Training precision\", str, \"bf16\", choices=[\"fp32\", \"bf16\", \"fp16_autocast\", \"bf16_autocast\", \"bf16_buffers_autocast\"])\n",
    "        self.model_name = Parameter('Which model to train - e.g. \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"', str, \"meta-llama/Llama-2-7b-hf\")\n",
    "        self.save_model = Parameter('Save the resulting model', bool, False)\n",
    "        self.output_dir = Parameter('Output directory to save the final model to', str, \"output\")\n",
    "        self.lora_rank = Parameter('LoRA rank for lora/qlora', int, 64)\n",
    "        self.lora_alpha = Parameter('LoRA alpha for lora/qlora', int, 16)\n",
    "        self.lora_dropout = Parameter('LoRA dropout for lora/qlora', float, 0.1)\n",
    "        self.lora_target_modules = Parameter(\"LoRA target modules\", str, \"all\", choices=[\"all\", \"default\"])\n",
    "        self.verbose = Parameter('Whether to print extra info for debugging', bool, False)\n",
    "        self.lr = Parameter('Learning rate', float, 1e-5)\n",
    "        self.apply_gradient_clipping = Parameter('Apply gradient norm clipping', bool, False)\n",
    "        self.grad_norm = Parameter('Gradient norm clipping', float, 0.3)\n",
    "        self.wd = Parameter('Weight decay', float, 0.1)\n",
    "        self.profile_memory = Parameter('Profile memory usage for the first few batches. Keep false for training. May increase memory usage.', bool, False)\n",
    "        self.optimizer = Parameter(\"Optimizer\", str, \"adamw\", choices=[\"adamw\", \"adam\", \"sgd\", \"adadelta\"])\n",
    "        self.lr_scheduler = Parameter(\"Learning Rate Scheduler\", str, \"constant\", choices=[\"constant\", \"linear\", \"cosine\"])\n",
    "        self.loading_workers = Parameter('Number of layers to load and quantize in parallel per GPU. Default of -1 uses heuristics to set worker count.', int, -1)\n",
    "        self.log_to = Parameter(\"Where to log output\", str, \"tqdm\", choices=[\"tqdm\", \"wandb\", \"stdout\"])\n",
    "        self.master_addr = Parameter('For distributed training', str, \"localhost\")\n",
    "        self.master_port = Parameter('For distributed training, must be the same for all processes', str, \"12355\")\n",
    "        self.seed = Parameter('Random seed', int, 42)\n",
    "        self.project_name = Parameter('For wandb logging', str, \"fsdp_qlora\")\n",
    "        self.name = Parameter('For wandb logging', str, None)\n",
    "        self.group = Parameter('For wandb logging', str, None)\n",
    "        self.entity = Parameter('For wandb logging', str, None)\n",
    "        self.n_bits = Parameter('passed to hqq', int, 4)\n",
    "        self.profile = Parameter('Whether to profile with torch.profiler', bool, False)\n",
    "        self.profiling_output = Parameter('Output file prefix for profiling', str, \"profiles\")\n",
    "        self.with_stack = Parameter('Output stacks for profiling. Note that setting export_memory_timeline will automatically export traces since `with_stack` must be true to profile memory.', bool, False)\n",
    "        self.with_shapes = Parameter('Output shapes for profiling. Can impact performance. Note that setting export_memory_timeline will automatically export traces since `with_shapes` must be true to profile memory.', bool, False)\n",
    "        self.export_trace = Parameter('Output trace for profiling', bool, True)\n",
    "        self.export_memory_timeline = Parameter('Output memory timeline for profiling', bool, False)\n",
    "        self.wait_steps = Parameter('Wait steps when running profiler. Only used if repeat != 0.', int, 0)\n",
    "        self.warmup_steps = Parameter('Warmup steps when running profiler', int, 1)\n",
    "        self.active_steps = Parameter('Active steps when running profiler', int, 2)\n",
    "        self.repeat = Parameter('Number of profiler cycles (wait + warmup + active) if > 0, else repeats forever', int, 0)\n",
    "        self.profiling_frequency = Parameter('Profiling frequency in steps. Only used if repeat == 0, in which case wait_steps will be set to profiling_frequency - (warmup_steps + active_steps) such that the effective cycle length == profiling_frequency', int, 10)\n",
    "        self.max_steps = Parameter('Max number of training steps (in units of batches) per epoch. -1 means no max_steps, otherwise training loop breaks after `max_steps` each epoch', int, -1)\n",
    "\n",
    "    def __str__(self): return \"\\n\".join([f\"{attr}: {getattr(self, attr).value}\" for attr in vars(self)])\n",
    "    def __repr__(self): return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Param>, __main__.Param)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = DefaultParameters(); params.batch_size, type(params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.batch_size.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Batch size per GPU. Effective BS = batch_size * world_size * gradient_accumulation_steps',\n",
       " None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.batch_size.description, params.batch_size.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Training type',\n",
       " ['full',\n",
       "  'lora',\n",
       "  'qlora',\n",
       "  'custom_qlora',\n",
       "  'custom_lora',\n",
       "  'hqq_lora',\n",
       "  'hqq_dora',\n",
       "  'bnb_dora',\n",
       "  'bnb_llama_pro',\n",
       "  'hqq_llama_pro'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.train_type.description, params.train_type.choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
